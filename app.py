import streamlit as st
import os
import requests

from utils.load_pdf import extract_text_from_pdf
from utils.load_docx import extract_text_from_docx
from utils.load_excel import extract_text_from_excel

# ---------------- CONFIGURA√á√ÉO ----------------
st.set_page_config(page_title="Agente IA com Hugging Face", layout="wide")
st.title("ü§ñ Agente de IA para An√°lise de Arquivos")
st.markdown("Escolha um arquivo e fa√ßa perguntas sobre seu conte√∫do!")

# ---------------- LISTA DE ARQUIVOS ----------------
data_dir = "data"
arquivos = [arq for arq in os.listdir(data_dir) if not arq.startswith('.')]

arquivo_escolhido = st.selectbox("üìÇ Escolha um arquivo:", arquivos)

# ---------------- LEITURA DO ARQUIVO ----------------
@st.cache_data(show_spinner=False)
def ler_arquivo(path):
    ext = path.split('.')[-1].lower()
    if ext == "pdf":
        return extract_text_from_pdf(path)
    elif ext == "docx":
        return extract_text_from_docx(path)
    elif ext in ["xls", "xlsx"]:
        return extract_text_from_excel(path)
    elif ext == "txt":
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        return None

# ---------------- FUN√á√ÉO PARA CONSULTAR A IA ----------------
def perguntar_para_huggingface(texto, pergunta):
    API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1"
    headers = {
        "Authorization": f"Bearer {st.secrets['huggingface']['api_key']}"
    }

    prompt = f"""Voc√™ √© um assistente que responde com base no conte√∫do de um documento.
Documento:
\"\"\"
{texto[:3000]}
\"\"\"

Pergunta: {pergunta}
Resposta:"""

    payload = {
        "inputs": prompt,
        "parameters": {
            "temperature": 0.3,
            "max_new_tokens": 200
        }
    }

    try:
        response = requests.post(API_URL, headers=headers, json=payload)

        if response.status_code != 200:
            return f"‚ùå Erro da API: {response.status_code} - {response.reason}"

        if not response.content:
            return "‚ùå A resposta da IA veio vazia. O modelo pode estar carregando ou indispon√≠vel no momento."

        resposta_json = response.json()

        if isinstance(resposta_json, list) and "generated_text" in resposta_json[0]:
            return resposta_json[0]["generated_text"].split("Resposta:")[-1].strip()
        elif "error" in resposta_json:
            return f"‚ùå Erro da IA: {resposta_json['error']}"
        else:
            return "‚ùå A resposta da IA n√£o p√¥de ser interpretada corretamente."

    except Exception as e:
        return f"‚ùå Erro inesperado ao conectar √† Hugging Face: {str(e)}"

# ---------------- INTERFACE PRINCIPAL ----------------
if arquivo_escolhido:
    caminho = os.path.join(data_dir, arquivo_escolhido)
    texto = ler_arquivo(caminho)

    if texto:
        with st.expander("üìÑ Ver conte√∫do extra√≠do (opcional)"):
            st.text_area("Conte√∫do do arquivo:", texto, height=300)

        pergunta = st.text_input("‚ùì Fa√ßa sua pergunta sobre o conte√∫do do arquivo:")

        if pergunta.strip():
            with st.spinner("Consultando a IA..."):
                resposta = perguntar_para_huggingface(texto, pergunta)
                st.success("üß† Resposta:")
                st.write(resposta)
    else:
        st.error("‚ùå N√£o foi poss√≠vel ler o arquivo selecionado.")
